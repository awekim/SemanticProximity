{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic proximity in quantum and general scientific publications: A BERTopic approach to mapping regional knowledge spaces\n",
    "- Made by: Keungoui Kim (Ph.D.) & Jisoo Hur (Ph.D.)  \n",
    "- Part 03. BERT-based Similarity Measurement\n",
    "- Data set: WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))  # 첫 번째 GPU 이름 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"H:/GD_awekimm/[HGU]/[Research]/12_허지수/00_SemanticProximity/SemanticProximity_research/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the datasets\n",
    "pub_data = pd.read_csv(dir+'pub_bertopic_chatgpt.csv')\n",
    "quantum_data = pd.read_csv(dir+'quantum_pub_bertopic_chatgpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eu_nuts_id</th>\n",
       "      <th>period</th>\n",
       "      <th>keyword</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['nomenclature', 'names', 'botanical', 'intern...</td>\n",
       "      <td>International Botanical Nomenclature and Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['galaxies', 'star', 'dust', 'galaxy', 'redshi...</td>\n",
       "      <td>Infrared Emission and Stellar Mass in Distant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['wireless', 'channel', 'modulation', 'mimo', ...</td>\n",
       "      <td>Advanced Wireless Communication Techniques for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['patients', 'cells', 'cell', 'species', 'expr...</td>\n",
       "      <td>Impact of Increased Cell Expression in Disease...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['bar', 'detector', 'gamma', 'decays', 'gt', '...</td>\n",
       "      <td>Gamma Decay Detection and Branching Ratios in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eu_nuts_id  period                                            keyword  \\\n",
       "0      UKM25       1  ['nomenclature', 'names', 'botanical', 'intern...   \n",
       "1      UKM25       1  ['galaxies', 'star', 'dust', 'galaxy', 'redshi...   \n",
       "2      UKM25       1  ['wireless', 'channel', 'modulation', 'mimo', ...   \n",
       "3      UKM25       1  ['patients', 'cells', 'cell', 'species', 'expr...   \n",
       "4      UKM25       1  ['bar', 'detector', 'gamma', 'decays', 'gt', '...   \n",
       "\n",
       "                                             content  \n",
       "0  International Botanical Nomenclature and Class...  \n",
       "1  Infrared Emission and Stellar Mass in Distant ...  \n",
       "2  Advanced Wireless Communication Techniques for...  \n",
       "3  Impact of Increased Cell Expression in Disease...  \n",
       "4  Gamma Decay Detection and Branching Ratios in ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Similairty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device='cuda') # all-MiniLM-L6-v2\n",
    "print(f\"Model is loaded on device: {model.device}\")\n",
    "\n",
    "regions = pub_data['eu_nuts_id'].unique()\n",
    "periods = pub_data['period'].unique()\n",
    "\n",
    "results = []\n",
    "for region in regions:\n",
    "    for period in periods:\n",
    "        \n",
    "        df1_filtered = pub_data[(pub_data['eu_nuts_id'] == region) & (pub_data['period'] == period)]\n",
    "        df2_filtered = quantum_data[(quantum_data['eu_nuts_id'] == region) & (quantum_data['period'] == period)]\n",
    "        \n",
    "        # Not much effective for this case\n",
    "        doc1 = [re.sub(r'<.*?>|[\"]', '', item) for item in df1_filtered['content'].to_list()]\n",
    "        doc2 = [re.sub(r'<.*?>|[\"]', '', item) for item in df2_filtered['content'].to_list()]\n",
    "\n",
    "        embeddings_a = model.encode(doc1, convert_to_tensor=True)  \n",
    "        embeddings_b = model.encode(doc2, convert_to_tensor=True)  \n",
    "\n",
    "        ### Compute cosine-similarities       \n",
    "        # Version 1\n",
    "        similarities = util.pytorch_cos_sim(embeddings_a, embeddings_b).cpu().numpy()\n",
    "        average_similarity = similarities.mean()\n",
    "        # Version 2\n",
    "        # similarities = []\n",
    "        # for embed_a in embeddings_a:  \n",
    "        #     for embed_b in embeddings_b:  \n",
    "        #         similarity = util.cos_sim(embed_a, embed_b).item()  \n",
    "        #         similarities.append(similarity)\n",
    "        # average_similarity = np.mean(similarities)\n",
    "\n",
    "        results.append({'eu_nuts_id': region, 'period': period, 'bert_similarity': average_similarity})\n",
    "\n",
    "similarity_df = pd.DataFrame(results)\n",
    "\n",
    "similarity_df.to_csv(dir+'similarity_bert_ed_250207.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Impact of Smoking on Asphalt Mixtures and Their Cessation Strategies in Environmental Contexts', 'Impact of Climate Change on Aquatic Ecosystems and Vegetation in Lake Deltas', 'Clinical Trials in Cellular Therapies for Disease Management in Patients', 'Quantum Dynamics of Gaussian States and Entanglement in Multidimensional Systems', 'Stellar Formation and Mass Distribution in Galaxies Through Redshift Surveys', 'Quantum Gravity and Cosmological Theories in String Dynamics']\n",
      "['Quantum Molecular Interactions in Tunneling and Adsorption Processes', 'Excitonic Dynamics in Polarization-Grown Quantum Wells of GaAs', 'Quantum Entropy Measures and Monogamy Relations in Steering Scenarios', 'Quantum Information Systems and Estimation Techniques', 'Quantum Cosmology and Galactic Dynamics', 'Dynamics of Nonequilibrium Interactions in Rydberg Atom Gases within Dissipative Lattice Systems', 'Quantum Antiferromagnetic Superfluidity and Magnetic Ratchet Effects in Fermionic Systems', '2D van der Waals Materials and Their Crystalline Properties', 'Self-Assembled Ionic Nanofibers for Enhanced Mobility in Graphene-Based Charge Carriers']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0876095"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check\n",
    "\n",
    "region='UKF14'\n",
    "period=4\n",
    "df1_filtered = pub_data[(pub_data['eu_nuts_id'] == region) & (pub_data['period'] == period)]\n",
    "df2_filtered = quantum_data[(quantum_data['eu_nuts_id'] == region) & (quantum_data['period'] == period)]\n",
    "\n",
    "# Not much effective for this case\n",
    "doc1 = [re.sub(r\"<(.*?)>\", r\"\\1\", item) for item in df1_filtered['content'].to_list()]\n",
    "doc1 = [item.replace('\"', '') for item in doc1]\n",
    "doc2 = [re.sub(r\"<(.*?)>\", r\"\\1\", item) for item in df2_filtered['content'].to_list()]\n",
    "doc2 = [item.replace('\"', '') for item in doc2]\n",
    "\n",
    "embeddings_a = model.encode(doc1, convert_to_tensor=True)  # A의 임베딩\n",
    "embeddings_b = model.encode(doc2, convert_to_tensor=True)  # B의 임베딩\n",
    "\n",
    "# Version 1\n",
    "similarities = util.pytorch_cos_sim(embeddings_a, embeddings_b).cpu().numpy()\n",
    "average_similarity = similarities.mean()\n",
    "# Version 2\n",
    "# similarities = []\n",
    "# for embed_a in embeddings_a:  \n",
    "#     for embed_b in embeddings_b:  \n",
    "#         similarity = util.cos_sim(embed_a, embed_b).item()  \n",
    "#         similarities.append(similarity)\n",
    "# average_similarity = np.mean(similarities)\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "print(average_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Old version ##################\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    # Get the output from BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get the embeddings for the [CLS] token (the first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "# Function to calculate BERT-based similarity\n",
    "def calculate_bert_similarity(df1, df2, region, period):\n",
    "    # Filter data by region and period\n",
    "    df1_filtered = df1[(df1['eu_nuts_id'] == region) & (df1['period'] == period)]\n",
    "    df2_filtered = df2[(df2['eu_nuts_id'] == region) & (df2['period'] == period)]\n",
    "    \n",
    "    # Check if both filtered datasets have data\n",
    "    if df1_filtered.empty or df2_filtered.empty:\n",
    "        return None  # Return None if there's no data for the given region and period\n",
    "    \n",
    "    # Concatenate the 'content' field to create a document for each set\n",
    "    doc1 = ' '.join(df1_filtered['content'].astype(str)).lower()\n",
    "    doc2 = ' '.join(df2_filtered['content'].astype(str)).lower()\n",
    "    \n",
    "    # Preprocess the documents by removing stopwords\n",
    "    doc1 = remove_stopwords(doc1)\n",
    "    doc2 = remove_stopwords(doc2)\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    embedding1 = get_bert_embedding(doc1)\n",
    "    embedding2 = get_bert_embedding(doc2)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    \n",
    "    return similarity[0][0]  # Return the similarity score\n",
    "\n",
    "# Apply similarity calculation for each unique combination of region and period\n",
    "regions = pub_data['eu_nuts_id'].unique()\n",
    "periods = pub_data['period'].unique()\n",
    "\n",
    "results = []\n",
    "\n",
    "for region in regions:\n",
    "    for period in periods:\n",
    "        bert_score = calculate_bert_similarity(pub_data, quantum_data, region, period)\n",
    "        results.append({'eu_nuts_id': region, 'period': period, 'bert_similarity': bert_score})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "similarity_df = pd.DataFrame(results)\n",
    "\n",
    "# Display or save the results\n",
    "print(similarity_df) \n",
    "\n",
    "similarity_df.to_csv('similarity_bert.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
